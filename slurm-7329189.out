/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.62:44527'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.62:39260'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.62:38417'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.62:40156'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.62:36071'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.62:44150'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.62:37178'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.62:37223'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.62:41448'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.62:38160'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.62:40455'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.62:33352'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.62:35526'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.62:42397'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.62:41282'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.62:41909'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.62:40936'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.62:44427'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.62:39846'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.62:37099'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.62:43768'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.62:37141'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.62:40503'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.62:36616'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.62:35031'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.62:43871'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.62:43157'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.62:35089'
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.8.62:39254
distributed.worker - INFO -          Listening to:    tcp://172.30.8.62:39254
distributed.worker - INFO -          dashboard at:          172.30.8.62:45251
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.8.62:37737
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:    tcp://172.30.8.62:37737
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.8.62:40737
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -          dashboard at:          172.30.8.62:38163
distributed.worker - INFO -          Listening to:    tcp://172.30.8.62:40737
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-hvan5gtr
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO -          dashboard at:          172.30.8.62:34250
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-0j5smxxv
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-j20io7ou
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.8.62:33168
distributed.worker - INFO -          Listening to:    tcp://172.30.8.62:33168
distributed.worker - INFO -          dashboard at:          172.30.8.62:42690
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.8.62:36610
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.30.8.62:36610
distributed.worker - INFO -          dashboard at:          172.30.8.62:36418
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-pm4qqdx8
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-orjr5cbc
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.8.62:36600
distributed.worker - INFO -          Listening to:    tcp://172.30.8.62:36600
distributed.worker - INFO -          dashboard at:          172.30.8.62:43780
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-bqkqkk6m
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.8.62:38301
distributed.worker - INFO -          Listening to:    tcp://172.30.8.62:38301
distributed.worker - INFO -          dashboard at:          172.30.8.62:45710
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-9uey8zf6
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.8.62:46312
distributed.worker - INFO -          Listening to:    tcp://172.30.8.62:46312
distributed.worker - INFO -          dashboard at:          172.30.8.62:37889
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-0z3geppm
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.8.62:41331
distributed.worker - INFO -          Listening to:    tcp://172.30.8.62:41331
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.8.62:34011
distributed.worker - INFO -          dashboard at:          172.30.8.62:41435
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO -          Listening to:    tcp://172.30.8.62:34011
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.30.8.62:45818
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-7yrqo5o_
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-7sj5gjxi
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.8.62:43125
distributed.worker - INFO -          Listening to:    tcp://172.30.8.62:43125
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.8.62:37772
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.8.62:42386
distributed.worker - INFO -          Listening to:    tcp://172.30.8.62:37772
distributed.worker - INFO -          dashboard at:          172.30.8.62:39343
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.8.62:46185
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO -          Listening to:    tcp://172.30.8.62:46185
distributed.worker - INFO -          dashboard at:          172.30.8.62:34322
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.8.62:36495
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO -          Listening to:    tcp://172.30.8.62:42386
distributed.worker - INFO -          dashboard at:          172.30.8.62:35956
distributed.worker - INFO -          dashboard at:          172.30.8.62:41796
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.8.62:32932
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-zkqfzkbc
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-62rso0f7
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-egfh_367
distributed.worker - INFO -          Listening to:    tcp://172.30.8.62:36495
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.30.8.62:35042
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-l9m2b70z
distributed.worker - INFO -          Listening to:    tcp://172.30.8.62:32932
distributed.worker - INFO -          dashboard at:          172.30.8.62:39891
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.8.62:36925
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.30.8.62:36925
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.8.62:36718
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-557c27ua
distributed.worker - INFO -          dashboard at:          172.30.8.62:44887
distributed.worker - INFO -          Listening to:    tcp://172.30.8.62:36718
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -          dashboard at:          172.30.8.62:36923
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-du497h75
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.8.62:39070
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.8.62:34995
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO -          Listening to:    tcp://172.30.8.62:39070
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.30.8.62:34995
distributed.worker - INFO -          dashboard at:          172.30.8.62:37454
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -          dashboard at:          172.30.8.62:44833
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-tkbbe9rp
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-je0nm8fi
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-yiowjk80
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-1pr0neos
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.8.62:37412
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.8.62:45598
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.8.62:36743
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.8.62:39482
distributed.worker - INFO -          Listening to:    tcp://172.30.8.62:37412
distributed.worker - INFO -          Listening to:    tcp://172.30.8.62:45598
distributed.worker - INFO -          Listening to:    tcp://172.30.8.62:36743
distributed.worker - INFO -          Listening to:    tcp://172.30.8.62:39482
distributed.worker - INFO -          dashboard at:          172.30.8.62:45257
distributed.worker - INFO -          dashboard at:          172.30.8.62:34432
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.8.62:33198
distributed.worker - INFO -          dashboard at:          172.30.8.62:42375
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO -          dashboard at:          172.30.8.62:39546
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO -          Listening to:    tcp://172.30.8.62:33198
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.30.8.62:35460
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-83xedmby
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-p6gun60o
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-meb9l5aw
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.8.62:33740
distributed.worker - INFO -          Listening to:    tcp://172.30.8.62:33740
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-i3fu5rrj
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:          172.30.8.62:41567
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-07mxn3sc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-38g3f8ii
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.8.62:43264
distributed.worker - INFO -          Listening to:    tcp://172.30.8.62:43264
distributed.worker - INFO -          dashboard at:          172.30.8.62:37998
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-clhruwtx
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.8.62:40099
distributed.worker - INFO -          Listening to:    tcp://172.30.8.62:40099
distributed.worker - INFO -          dashboard at:          172.30.8.62:33627
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-n_e_tht2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:44930
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:44930
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:44930
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.utils_perf - INFO - full garbage collection released 32.09 MB from 20933 reference cycles (threshold: 10.00 MB)
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:44930
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.1:44930'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.62:37772
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.1:44930'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.62:36610
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.1:44930'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.62:36495
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.1:44930'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.62:39482
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.1:44930'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.62:34011
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.1:44930'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.62:37737
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.1:44930'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.62:36925
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.1:44930'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.62:40099
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.1:44930'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.62:41331
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.1:44930'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.62:37412
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.1:44930'
NoneType: None
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.1:44930'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.62:39254
distributed.worker - INFO - Stopping worker at tcp://172.30.8.62:32932
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.1:44930'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.62:40737
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.1:44930'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.62:36743
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.1:44930'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.62:46185
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.1:44930'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.62:45598
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.1:44930'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.62:33198
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.1:44930'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.62:38301
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.1:44930'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.62:36600
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.1:44930'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.62:36718
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.1:44930'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.62:34995
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.1:44930'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.62:43125
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.1:44930'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.62:43264
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.1:44930'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.62:33740
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.1:44930'
NoneType: None
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.1:44930'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.62:33168
distributed.worker - INFO - Stopping worker at tcp://172.30.8.62:42386
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.62:43768'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.62:40455'
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.1:44930'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.62:46312
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.62:44427'
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.1:44930'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.62:39070
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.62:40156'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.62:41448'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.62:33352'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.62:44150'
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.62:40936'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.62:37178'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.62:37141'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.62:38160'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.62:36071'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.62:43871'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.62:35089'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.62:44527'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.62:41909'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.62:38417'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.62:43157'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.62:35526'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.62:42397'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.62:39846'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.62:41282'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.62:40503'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.62:35031'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.62:39260'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.62:37099'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.62:36616'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.62:37223'
distributed.dask_worker - INFO - End worker
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
